---

layout:     post
title:      "TensorFlow In a Nutshell 第三节：各类模型介绍"
subtitle:   ""
date:       2017-02-06
author:     "wykhan"
header-img: "img/post-bg-os-metro.jpg"
catalog: tool
tags:
    - nutshell
    - 入门
---


**TensorFlow In a Nutshell**

> The fast and easy guide to the most popular Deep Learning framework in the world

![Alt text](/img/tensorflow_nutshell-1-432x270.png)

[原文地址](http://camron.xyz/index.php/2016/09/13/hybrid_learning/),[代码地址](https://github.com/c0cky/TensorFlow-in-a-Nutshell/tree/master/part2)

这一节讲述tensorflow的各类模型，并给出使用示例和代码

___

##递归神经网络RNN

![picture]()

适用领域：语言模型，机器翻译，词嵌入，文本处理
借助LSTM和GRU的出色表现，RNN在自然语言处理领域完成了一次飞跃。将字符转化为向量作为输入，它就可以根据训练集产生新的句子。这个模型的优点在于它保留了句子的上下文，并派生出“猫坐在垫子上”的意思，意思是猫在垫子上。tensorflow的出现，让建立这些网络已经变得越来越简单。这里有具体的[例子](http://www.wildml.com/2016/08/rnns-in-tensorflow-a-practical-guide-and-undocumented-features/)。

```python
import tensorflow as tf 
import numpy as np
```  

```python
# Create input data
X = np.random.randn(2, 10, 8)

# The second example is of length 6 
X[1,6,:] = 0
X_lengths = [10, 6]

cell = tf.nn.rnn_cell.LSTMCell(num_units=64, state_is_tuple=True)
cell = tf.nn.rnn_cell.DropoutWrapper(cell=cell, output_keep_prob=0.5)
cell = tf.nn.rnn_cell.MultiRNNCell(cells=[cell] * 4, state_is_tuple=True)

outputs, last_states = tf.nn.dynamic_rnn(
    cell=cell,
    dtype=tf.float64,
    sequence_length=X_lengths,
    inputs=X)

result = tf.contrib.learn.run_n(
    {"outputs": outputs, "last_states": last_states},
    n=1,
    feed_dict=None)
```
  
___

##卷积神经网络CNN
![picture]()
适用领域：图像处理，人脸识别，计算视觉
CNN很独特，它被设计为用图像做输入。并有一个滑动窗口，窗口在矩阵上移动，产生卷积特征。

 
详情略（非NLP问题，偷懒了，需要的话看原文）

___

##前馈神经网络Feed Forward Neural Networks
![picture]()
适用领域：分类和回归
这种网络由多个感知层组成，感知层将信息逐层向下传播，最后一层产生输出结果。同一层的节点彼此之间没有连接。除了输入层和输出层，中间的都叫做隐层。
这个网络的目标和其他有监督的神经网络类似，采用后向传播，使得输入产生其预期的输出。它是解决分类和回归问题最简单而有效的神经网络。
看个手写体识别的例子，数据为TF官方示例中的数据。

```python
def init_weights(shape):
    return tf.Variable(tf.random_normal(shape, stddev=0.01))
```  

```python
def model(X, w_h, w_o):
    h = tf.nn.sigmoid(tf.matmul(X, w_h)) # this is a basic mlp, think 2 stacked logistic regressions
    return tf.matmul(h, w_o) # note that we dont take the softmax at the end because our cost fn does that for us
```  

```python
mnist = input_data.read_data_sets("MNIST_data/", one_hot=True)
trX, trY, teX, teY = mnist.train.images, mnist.train.labels, mnist.test.images, mnist.test.labels
```  
```python
X = tf.placeholder("float", [None, 784])
Y = tf.placeholder("float", [None, 10])
```  
```python
w_h = init_weights([784, 625]) # create symbolic variables
w_o = init_weights([625, 10])
```  

```python
py_x = model(X, w_h, w_o)
```  
```python
cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(py_x, Y)) # compute costs
train_op = tf.train.GradientDescentOptimizer(0.05).minimize(cost) # construct an optimizer
predict_op = tf.argmax(py_x, 1)
```  

```python
# Launch the graph in a session
with tf.Session() as sess:
    # you need to initialize all variables
    tf.initialize_all_variables().run()
```  
```python
for i in range(100):
        for start, end in zip(range(0, len(trX), 128), range(128, len(trX)+1, 128)):
            sess.run(train_op, feed_dict={X: trX[start:end], Y: trY[start:end]})
        print(i, np.mean(np.argmax(teY, axis=1) ==
                         sess.run(predict_op, feed_dict={X: teX, Y: teY})))
```  

___

##线性模型
![picture]()

适用领域：分类和回归
线性模型根据给定的X值，拟合出最适合Y的一条直线。例如，假设我们有某个街区的住宅面积和总价的数据，我们就可以建立一个线性模型，用面积来预测总价。
需要注意的一点是线性模型可以有多个X特征。比如房价预测的例子，输入除了面积，还可以包括地段，朝向，是否学区房等。

```python
import numpy as np
import tensorflow as tf
```  
```python
def weight_variable(shape):
    initial = tf.truncated_normal(shape, stddev=1)
    return tf.Variable(initial)
```  
```python
# dataset
xx = np.random.randint(0,1000,[1000,3])/1000.
yy = xx[:,0] * 2 + xx[:,1] * 1.4 + xx[:,2] * 3
```  
```python
# model
x = tf.placeholder(tf.float32, shape=[None, 3])
y_ = tf.placeholder(tf.float32, shape=[None])
W1 = weight_variable([3, 1])
y = tf.matmul(x, W1)
```  
```python
# training and cost function
cost_function = tf.reduce_mean(tf.square(tf.squeeze(y) - y_))
train_function = tf.train.AdamOptimizer(1e-2).minimize(cost_function)
```  
```python
# create a session
sess = tf.Session()
```  
```python
# train
sess.run(tf.initialize_all_variables())
for i in range(10000):
    sess.run(train_function, feed_dict={x:xx, y_:yy})
    if i % 1000 == 0:
        print(sess.run(cost_function, feed_dict={x:xx, y_:yy}))
```  

___

##支持向量机SVM
![picture]()
用途：二值分类，也可以基于二值做多值分类
SVM的总体思路就是找最优分类的超平面，让支持向量距离超平面的距离尽可能的大。如果数据不是线性可分的，就通过核函数将其转化到新的空间。SVM是个很优美的算法，即使特征维数大于样本数量，它也依然可以表现良好。

```python
def input_fn():
      return {
          'example_id': tf.constant(['1', '2', '3']),
          'price': tf.constant([[0.6], [0.8], [0.3]]),
          'sq_footage': tf.constant([[900.0], [700.0], [600.0]]),
          'country': tf.SparseTensor(
              values=['IT', 'US', 'GB'],
              indices=[[0, 0], [1, 3], [2, 1]],
              shape=[3, 5]),
          'weights': tf.constant([[3.0], [1.0], [1.0]])
      }, tf.constant([[1], [0], [1]])
```

```python
price = tf.contrib.layers.real_valued_column('price')
    sq_footage_bucket = tf.contrib.layers.bucketized_column(
        tf.contrib.layers.real_valued_column('sq_footage'),
        boundaries=[650.0, 800.0])
    country = tf.contrib.layers.sparse_column_with_hash_bucket(
        'country', hash_bucket_size=5)
    sq_footage_country = tf.contrib.layers.crossed_column(
        [sq_footage_bucket, country], hash_bucket_size=10)
    svm_classifier = tf.contrib.learn.SVM(
        feature_columns=[price, sq_footage_bucket, country, sq_footage_country],
        example_id_column='example_id',
        weight_column_name='weights',
        l1_regularization=0.1,
        l2_regularization=1.0)
```
```python
svm_classifier.fit(input_fn=input_fn, steps=30)
    accuracy = svm_classifier.evaluate(input_fn=input_fn, steps=1)['accuracy']
```

___

##深度广度混合模型Deep and Wide Models
![picture]()
适用领域：推荐系统，分类和回归
这个模型在第二节介绍过了，它的特点是较少的特征工程，不错的预测结果。

___

##随机森林Random Forest
![picture]()
适用领域：分类和回归
森林中的每一颗树都是一个独特的分类器，所有树共同投票，最终选用票数最多的结果。很像雅典元老院的决策机制。
随机森林的神奇之处是其不会过拟合，树的数量多多益善。
用尾花卉数据集（Iris）做示例

```python
hparams = tf.contrib.tensor_forest.python.tensor_forest.ForestHParams(
        num_trees=3, max_nodes=1000, num_classes=3, num_features=4)
classifier = tf.contrib.learn.TensorForestEstimator(hparams)
```
```python
iris = tf.contrib.learn.datasets.load_iris()
data = iris.data.astype(np.float32)
target = iris.target.astype(np.float32)
```
```python
monitors = [tf.contrib.learn.TensorForestLossMonitor(10, 10)]
classifier.fit(x=data, y=target, steps=100, monitors=monitors)
classifier.evaluate(x=data, y=target, steps=10)
```

___




